{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_learning_101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIIXwhc7GCUT"
      },
      "source": [
        "Probeer deze eerste cell te runnen om te testen of je notebook werkt. Als deze de output Hello world geeft, werkt het.\r\n",
        "(je kan een cell uitvoeren door deze te selecteren en SHIFT+ENTER te gebruiken, of door op de play knop te klikken)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7rnLhNjxbJ"
      },
      "source": [
        "print('Hello world!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf-KNV4qGJKx"
      },
      "source": [
        "**Jupyter notebooks**\r\n",
        "Wat heb ik voor me? Dit is een Jupyter notebook. Dit is eigenlijk een omgeving waarin je python code stukje voor stukje kan draaien. Variabelen en functies worden bewaard tussen de cellen, dus je hoeft niet steeds je hele script opnieuw te draaien wanneer je een nieuw stukje code hebt gegenereerd. Dit notebook bestaat uit markdown cellen met tekst en uitleg, en code cellen met python code. Elke code cell heeft \"In [ ]:\" er voor staan. De code die je hier invoert, wordt als input gezien, direct onder de cell verschijnt de output.\r\n",
        "\r\n",
        "_Jupyter heeft bepaalde styling ingebouwd. Als je aan het eind van je cell een bepaalde waarde wilt outputten, hoef je deze niet tussen print() statement te zetten, zolang je de naam van de variabele gewoon op de laatste regel van je cell plaatst. In sommige gevallen, zoals met pandas dataframes, zal je output er mooier uit zien zonder print statement, omdat jupyter automatisch styling toepast. (let wel op, als je je jupyter code naar ene python document kopieert zal de code in de war raken van willekeurige rijen met alleen de naam van een variabele er in)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBOyknnqGQcB"
      },
      "source": [
        "# De Titanic\r\n",
        "In 1912 verging de Titanic als gevolg van een ijsberg. Nu, ruim 100 jaar later, kunnen we terugkijken op de slachtoffers en overlevenden. Er is openbare data beschikbaar over de passagiers en hun lot, en deze kunnen we gebruiken om te voorspellen wie de reis wel of niet zou overleven als ze er bij waren geweest.\r\n",
        "\r\n",
        "In deze workshop gaan we aan de hand van de [Titanic dataset](https://public.opendatasoft.com/explore/dataset/titanic-passengers/export/) doornemen hoe machine learning werkt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLxPrTsZj2Ui"
      },
      "source": [
        "# processing\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# visualisation\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# machine learning\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ncxeU-GYVd"
      },
      "source": [
        "We gaan nu beginnen met het maken van ons model. Om te beginnen hebben we een aantal libraries nodig. Pandas en numpy voor het werken met een dataset en wiskundige berekeningen, matplotlib en seaborn voor visualisatie van data, en sklearn voor voorgebouwde algoritmes en databewerking.\r\n",
        "\r\n",
        "Zoals je ziet geef ik de meeste libraries die ik importeer direct een alias. Deze zijn allemaal volgens standaard best practices, dus deze afkortingen zul je vaak tegenkomen op internet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNCMlfGaGa0O"
      },
      "source": [
        "# Extract\r\n",
        "We beginnen met het inladen van onze data. Er zijn meerdere versies van de Titanic dataset te vinden. Globaal bevatten ze allemaal ongeveer de zelfde kolommen en waardes. Wij gebruiken deze: [bron](https://public.opendatasoft.com/explore/dataset/titanic-passengers/export/). Ik heb de csv al toegevoegd aan deze repository, dus we gaan de dataset importeren uit onze eigen files.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhLEv4C8E9KM"
      },
      "source": [
        "df = pd.read_csv('titanic-passengers.csv', delimiter=';')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muI7tVR8G91D"
      },
      "source": [
        "## Inventarisatie van de data\r\n",
        "`df.head()` geeft ons de eerste 5 rijen data uit de dataset. Hierin zien we alle kolommen, en direct wat voorbeelden van wat voor soort data er in zit.\r\n",
        "- PassengerId: uniek id voor elke passagier\r\n",
        "- Survived\r\n",
        "- Pclass: de klasse waarin de passagier heeft gereisd (1, 2 of 3)\r\n",
        "- Name\r\n",
        "- Sex\r\n",
        "- Age\r\n",
        "- SibSp: aantal siblings / spouses aan boord\r\n",
        "- Parch: aantal parents / children aan boord\r\n",
        "- Ticket: ticketnummer\r\n",
        "- Fare: het bedrag wat de passagier heeft betaald\r\n",
        "- Cabin: kamernummer waar de passagier verbleef, hieruit kunnen we halen welke Deck een passagier zat\r\n",
        "- Embarked: de haven waar de passagier aan boord van het schip is gekomen\r\n",
        "\r\n",
        "## Wat zijn onze verwachtingen? \r\n",
        "Nu we weten welke data we hebben, wat verwachten we dat de grootste impact zal hebben? We kennen allemaal het verhaal van de Titanic, hebben mogelijk de film gezien. Op basis van deze voorkennis, kunnen we onze eerste hypotheses opstellen. We verwachten bijvoorbeeld dat de volgende factoren de groote impact zullen hebben op de overlevingskans van een passagier: leeftijd, geslacht, en klasse.\r\n",
        "Deze hypotheses zullen we zometeen checken.\r\n",
        "\r\n",
        "\r\n",
        "## Een eerste verkenning van de data\r\n",
        "Voordat we de data gaan analyseren, willen we checken of datatypes goed zijn geïmporteerd:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sdl0M6FFHCH"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVQzcAD1FJpm"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVrtwS2CFLKV"
      },
      "source": [
        "df[['Survived', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cgQUVChHGVl"
      },
      "source": [
        "### Verdeling van waardes over de data\r\n",
        "`df.describe()` geeft wat snelle geaggregeerde data per numerieke kolom. We zien het aantal waardes (waaruit opvalt dat `Age` een paar NULL (NaN) waardes heeft), het gemiddelde, standaarddeviatie, minimum en maximum, en het 25e, 50e en 75e percentiel. Zo weten we hoe de verdeling is van de waardes. We kunnen hier een aantal conclusies uit trekken over de datakwaliteit:\r\n",
        "\r\n",
        "- Passenger Id: dit zegt ons weinig. Een id geeft geen specifieke waarde over een passagier (passagier 20 is bijvoorbeeld niet tweemaal zo veel waard als passagier 10)\r\n",
        "- Passenger Class: het 50e percentiel heeft al de zelfde waarde als de maximum waarde. Hieraan zien we dat meer dan de helft van de pasagiers 3e klas reisde.\r\n",
        "- Age: de count is hier afwijkend van de rest, er zitten dus een aantal missende waardes in deze kolom.\r\n",
        "- SibSp: we zien dat ruim de helft van de passagiers geen siblings of spouses aan boord had. De afstand tussen het 95e percentiel en de maximale waarde is erg groot, dus er zijn enkele outliers, maar de meeste mensen hebben geen of 1 sibling/spouse bij zich.\r\n",
        "- Parch: we zien dat weinig mensen hun kinderen of ouders aan boord hadden, met enkele uitschieters.\r\n",
        "- Fare: we zien dat er passagiers zijn die een fare hebben betaald van 0, mogelijk waren dit werknemers op het schip? De gemmiddelde fare was 32, met een standaarddeviatie van 49. Dit suggereert dat er veel verschillen zaten tussen de fares van passagiers. We zien ook dat het 75e percentiel al bijna op de waarde van het gemmiddelde zit. Dit geeft aan dat het gemmiddelde ver omhoog wordt getrokken dankzij enkele hoge outliers.\r\n",
        "- Survived geeft aan dat het meest voorkomende geval \"No\" is, ofwel een meerderheid van 549 passagiers heeft de reis niet overleefd.\r\n",
        "- Sex laat ons zien dat er iets meer mannelijke passagiers aanwezig waren dan vrouwelijke.\r\n",
        "- De Cabin kolom mist veel data. We hebben maar 204 rijen beschikbaar, dus dat is lastig te corrigeren. Deze data zullen we niet kunnen gebruiken. \r\n",
        "- Embarked mist twee waardes. Deze zullen we later op kunnen vullen.\r\n",
        "\r\n",
        "Nu we deze waardes hebben gezien, kunnen we nog verder duiken in de verdeling van de waardes. Zoals je ziet hebben we hierboven in de `.describe()` onderscheid gemaakt tussen numerieke en tekstuele kolommen. Om te kunnen kijken naar al deze kolommen t.o.v. de overleving van de passagier, maken we een nieuwe kolom waarin we Survived omzetten naar een getal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBISeuG4FMAa"
      },
      "source": [
        "df.loc[df['Survived'] == \"Yes\", 'alive'] = 1\r\n",
        "df.loc[df['Survived'] == \"No\", 'alive'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18RJ3J-OFNys"
      },
      "source": [
        "pd.plotting.scatter_matrix(df, figsize=(15,15), alpha=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erh63ILvJrgy"
      },
      "source": [
        "Uit deze scatterplots en histogrammen komen een aantal interessante waarnemingen naar boven. We zien terug dat \r\n",
        "- de meerderheid de ramp niet overleefd heeft (rechtsonder)\r\n",
        "- de Fare inderdaad vooral concentreert rond lage prijzen, maar enkele uitschieters heeft boven de 400\r\n",
        "- we zien dat tussen Age en Pclass, de derde klas passagiers over het algemeen een lagere leeftijd hebben dan de eerste klas passagiers\r\n",
        "\r\n",
        "Dat laatste gaan we direct controleren:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDSdRHqHFO6U"
      },
      "source": [
        "df[['Pclass', 'Age']].groupby(['Pclass']).agg(['mean', 'min', 'max', 'std', 'var'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emIdiSa4J49F"
      },
      "source": [
        "De gemmiddelde leeftijd (de mean) van derde klas passagiers ligt meer dan 10 jaar lager dan eerste klas! Daarnaast zien we dat de oudste passagier (80 jaar) eerste klas reisde. Aan de standaarddeviatie zien we dat er onder de eerste klas meer variatie in leeftijd bestond dan onder de derde klas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsHxJRxpKC0n"
      },
      "source": [
        "### Hypotheses controleren\r\n",
        "We hadden ingeschat dat leeftijd, geslacht en klasse grote voorspellers zouden zijn voor overlevingskans. Klopt dat uit de data? We maken een paar grafieken om dat te testen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsBP2JP0FRk4"
      },
      "source": [
        "print(df[['Pclass', 'alive']].groupby(['Pclass']).mean())\r\n",
        "plt.figure()\r\n",
        "df['Pclass'].loc[df['alive'] == 0].hist(alpha=0.4, label='died')\r\n",
        "df['Pclass'].loc[df['alive'] == 1].hist(alpha=0.4, label='survived')\r\n",
        "plt.title('Pclass distribution over survival')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbvIl3UBFStX"
      },
      "source": [
        "df[['Sex', 'alive']].groupby(['Sex']).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bf6e05OFdq-"
      },
      "source": [
        "df.loc[df['Age'] >= 18, 'adult'] = 'Yes'\r\n",
        "df.loc[df['Age'] < 18, 'adult'] = 'No'\r\n",
        "print(df[['adult', 'alive']].groupby(['adult']).agg(['count', 'mean']))\r\n",
        "plt.figure()\r\n",
        "df['Age'].loc[df['alive'] == 0].hist(alpha=0.4, label='died')\r\n",
        "df['Age'].loc[df['alive'] == 1].hist(alpha=0.4, label='survived')\r\n",
        "plt.title('Age distribution over survival')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MEIMuMcL9xb"
      },
      "source": [
        "Op basis van de data lijken onze hypotheses te kloppen. \r\n",
        "\r\n",
        "**Klasse**\r\n",
        "Van de eerste klas passagiers heeft een grote meerderheid de ramp overleefd, in de tweede en derde klas verschuift deze meerderheid naar de niet-overlevers. Voornamelijk de derde klasse passagiers liepen risico: maar 24% van hen heeft overleefd.\r\n",
        "\r\n",
        "**Geslacht**\r\n",
        "Vrouwen kregen voorrang op de reddingsboten, en dat zien we. Van de vrouwen heeft bijna 75% het overleefd, onder de mannen was het percentage overlevers maar 18%.\r\n",
        "\r\n",
        "**Leeftijd**\r\n",
        "We schatten in dat kinderen, net als vrouwen, voorrang kregen in de reddingsboten. Dit kunnen we het makkelijkst procentueel weergeven door een splitsing te maken op volwassenheid, dus we hebben een kolom `adult` gemaakt, op leeftijd hoger of lager dan 18 jaar oud. Hieruit blijkt dat bijna 54% van de kinderen de ramp heeft ovrleefd, en maar 38% onder volwassenen.\r\n",
        "\r\n",
        "Onze hypotheses lijken dus te kloppen uit de data. We zullen straks zien of het model deze zelfde kenmerken aan zal wijzen als belangrijkste invloeden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmbXBbzNG2M"
      },
      "source": [
        "# Transform\r\n",
        "\r\n",
        "Nu komen we bij de volgende fase. We hebben gezien dat we in sommige kolommen wat data missen, soms zoveel dat de data onbruikbaar wordt. Daarnaast hebben we kolommen gezien met data die voor ons niet relevant is, ook deze zullen we verwijderen.\r\n",
        "\r\n",
        "In sommige gevallen kunnen we missende data wel opvullen, en daar zullen we dat doen.\r\n",
        "\r\n",
        "## Data cleanen\r\n",
        "\r\n",
        "### Niet relevante data\r\n",
        "\r\n",
        "We hebben gezien dat de Passenger_id kolom voor ons niet relevant is. Deze zullen we verwijderen uit het dataframe. Dit zelfde geldt voor het ticketnummer.\r\n",
        "\r\n",
        "### Missende data\r\n",
        "\r\n",
        "Missende data kunnen we op verschillende manieren behandelen:\r\n",
        "- De waarde alsnog achterhalen: is het mogelijk deze waardes alsnog te vinden, bij de bron van je dataset of op internet?\r\n",
        "- De waarde achterhalen uit andere data, mogelijk zit er in een andere kolom informatie waarmee je deze waarde alsnog kan vinden.\r\n",
        "- De waarde opvullen. Dit kan mogelijk uit de andere waardes in de zelfde kolom. Zo zou je bijvoorbeeld het gemmiddelde of de modus (meest voorkomende waarde) of de mediaan (middelste waarde) kunnen gebruiken, en daarmee de missende velden vullen.\r\n",
        "- De hele rij verwijderen: als deze kolom erg relevant is voor je data, en je kan de waarde niet met genoeg vertrouwen opvullen, is het misschien nodig om de hele rij te verwijderen. Als er in de kolom in te veel rijen waardes missen, is het lastig die op te vullen en kan de kolom mogelijk niet worden meegenomen in het model.\r\n",
        "\r\n",
        "De kolom Embarked mist maar een paar waardes. We kunnen niet achterhalen in welke haven een passagier is opgestapt, maar in dit geval kunnen we veilig genoeg de waarde opvullen met de modus van de kolom.\r\n",
        "\r\n",
        "De kolom Cabin mist een grote meerderheid van de waardes. We kunnen dit niet achterhalen uit een andere kolom, we weten niet genoeg om de waardes op te vullen met de modus of mediaan, dus we zullen deze kolom verwijderen.\r\n",
        "\r\n",
        "De kolom Age mist ook veel waardes. Deze kolom is voor ons waarschijnlijk erg interessant, dus deze willen we niet verwijderen. We zullen deze in dit geval opvullen met het gemmiddelde van de kolom. (Kun je nog andere tactieken bedenken? Probeer die eens uit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LatPKWP1FnEM"
      },
      "source": [
        "df.drop(['PassengerId'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE2x-Mj3Rxoy"
      },
      "source": [
        "df.drop(['Ticket'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bAf9t3BPPOg"
      },
      "source": [
        "df.drop(['Cabin'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-V6NQQcFkEw"
      },
      "source": [
        "df['Embarked'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvUEArNDFmD0"
      },
      "source": [
        "df['Embarked'] = df['Embarked'].fillna('S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ2vqxyQFpH1"
      },
      "source": [
        "df['Age'] = df['Age'].fillna(df['Age'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF9yfj2fPUp8"
      },
      "source": [
        "## Data voorbereiden\r\n",
        "\r\n",
        "Nu de data schoon is, moeten we nog wat bewerkingen doen om deze in het model te kunnen laden. Hierin zijn in dit geval twee stappen relevant:\r\n",
        "\r\n",
        "### One-Hot-Encoding\r\n",
        "We hebben een paar kolommen in tekstvorm. Een model kan niet zomaar rekenen met tekst, dus deze willen we omzetten naar numerieke waardes. Dit kunnen we echter niet zomaar doen met id's. Een voorbeeld: stel we hebben data gelinkt aan een tijdsverloop, met daarin een kolom `dag_van_de_week`. We kunnen dit niet zomaar omzetten naar `maandag = 1, dinsdag = 2, etc..`. Dinsdag is namelijk niet tweemaal zoveel waard als maandag, en je kan niet dinsdag maal 3 doen om op zaterdag uit te komen.\r\n",
        "\r\n",
        "Daarom zouden we de kolom met de dag van de week omzetten naar een nieuwe kolom voor elke categorie. Zo krijgen we nieuwe kolommen: [dvdw_maandag, dvdw_dinsdag, dvdw_woensdag, etc..]. In elk van deze kolommen staat een 0 of 1, afhankelijk van welke waarde voor die rij relevant is. Wanneer een bepaalde rij dus bij maandag hoort, is de eerste kolom een 1, en alle andere zijn 0. De originele kolom met de labels gebruiken we niet meer.\r\n",
        "\r\n",
        "Nadat we dit hebben gedaan, moeten we nog één stap nemen: het verwijderen van exact 1 van de nieuwe kolommen. Zo zouden we in het voorbeeld bijvoorbeeld dvdw_zondag weghalen. Dit voorkomt afhankelijkheid onder onze onafhankelijke variabelen. Als we alle kolommen laten bestaan, zal uit de waardes in 6 kolommen altijd met zekerheid de waarde van de 7e kolom blijken. Dit verstoort ons model.\r\n",
        "\r\n",
        "Dit hele proces noemen we **One-Hot-Encoding**.\r\n",
        "\r\n",
        "In het voorbeeld van de titanic hebben we een aantal tekst kolommen: 'Name', 'Sex', 'Embarked'. Binnen de kolom Name hebben we erg veel verschillende waardes, en de naam van een persoon zal weinig zeggen over zijn of haar overlevingskans, dus deze kolom zullen we niet gebruiken. (Kun jij een manier bedenken om deze kolom wel relevant te maken? Probeer het uit en test de invloed op het model!)\r\n",
        "\r\n",
        "De kolom Sex kunnen we eenvoudig omzetten naar een numerieke kolom `female` met daarin 1 of 0.\r\n",
        "\r\n",
        "De kolom Embarked bevat drie verschillende waardes. Deze zullen we omzetten naar nieuwe kolommen, met een pandas methode die hiervoor gemaakt is.\r\n",
        "\r\n",
        "Daarnaast hebben we numerieke kolommen die geen numerieke waarde representeren: bijvoorbeeld Pclass.\r\n",
        "Klasse 1 is niet de helft van klasse 2 waard (als deze verhouding al bestaat, dan zou die andersom zijn. Wil je testen of deze bestaat? Sla dan een stap over hier en test het model).\r\n",
        "We zullen nu Pclass behandelen als tekstkolom en deze ook omzetten naar nieuwe kolommen per categorie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy-BWdrZSpOe"
      },
      "source": [
        "df.drop(['Name'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgqzm2PNFrne"
      },
      "source": [
        "gendermap = {'male': 0, 'female': 1}\r\n",
        "df['female'] = df['Sex'].map(gendermap)\r\n",
        "df.drop(['Sex'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzzSjhWFux1"
      },
      "source": [
        "df = pd.get_dummies(df, prefix=['Embarked'], columns=['Embarked'])\r\n",
        "df.drop(['Embarked_C'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz7KbtyaFvqf"
      },
      "source": [
        "df = pd.get_dummies(df, prefix=['Pclass'], columns=['Pclass'])\r\n",
        "df.drop(['Pclass_1'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSWdALF2emCc"
      },
      "source": [
        "# save copy for later\r\n",
        "df2 = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F4B_O9-TLEy"
      },
      "source": [
        "# We hebben tijdens het verkennen en cleanen van de data een paar nieuwe kolommen toegevoegd, die we nu niet zullen gebruiken, dus die halen we weer weg.\r\n",
        "df.drop(['alive'], axis=1, inplace=True)\r\n",
        "df.drop(['adult'], axis=1, inplace=True)\r\n",
        "\r\n",
        "# Laten we kijken wat er uiteindelijk van onze dataset overblijft.\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rBReFELUJXi"
      },
      "source": [
        "# Model trainen\r\n",
        "\r\n",
        "Nu is het zo ver! We gaan een model trainen. Om dit te doen, maken we eerst de splitsing tussen onze training en test data.\r\n",
        "\r\n",
        "We gaan uit van een seed van 7. Dit is willekeurig, en kun je aanpassen. We gebruiken nu een vast getal, zodat we allemaal de zelfde splitsing krijgen, maar je kan deze vervangen door een ander getal, of door een random nummer, om andere willekeurige splitsingen te krijgen.\r\n",
        "\r\n",
        "Daarnaast gaan we voor een test_size van 20%. Ook dit kun je eventueel aanpassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz1lwS2iUk8d"
      },
      "source": [
        "# split data into training features and labels\r\n",
        "X, y = df.loc[:, df.columns != 'Survived'], df['Survived']\r\n",
        "\r\n",
        "# split data into train and test sets\r\n",
        "seed = 7\r\n",
        "test_size = 0.2\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0nPXIYaZHM_"
      },
      "source": [
        "## Decision Tree\r\n",
        "In dit geval gebruiken we een decision tree classifier. Dit algoritme maakt een beslisboom model waaruit een zo goed mogelijke splitsing komt. Dit is een van de modellen die al is ingebouwd in `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNwXWPsVWYW"
      },
      "source": [
        "decision_tree = DecisionTreeClassifier()\r\n",
        "decision_tree.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LjRgGO5ZV-6"
      },
      "source": [
        "## Voorspellen\r\n",
        "Nu we een getraind model hebben, kunnen we een voorspelling maken op onze test_data, om te kijken hoe goed het model presteert. We maken eerst een voorspelling, en daarna plaatsen we deze in een confusion matrix. Hierin kunnen we het aantal True Positives, True Negatives, False Positives en False Negatives lezen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bEvdyvrVr47"
      },
      "source": [
        "Y_pred = decision_tree.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8lTf9uaFpa"
      },
      "source": [
        "decision_tree.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JVMDPZNV_Xk"
      },
      "source": [
        "titles_options = [(\"Confusion matrix, without normalization\", None),\r\n",
        "                  (\"Normalized confusion matrix\", 'true')]\r\n",
        "for title, normalize in titles_options:\r\n",
        "    disp = plot_confusion_matrix(decision_tree, X_test, y_test,\r\n",
        "                                 display_labels=['Yes', 'No'],\r\n",
        "                                 cmap=plt.cm.Blues,\r\n",
        "                                 normalize=normalize)\r\n",
        "    disp.ax_.set_title(title)\r\n",
        "\r\n",
        "    print(title)\r\n",
        "    print(disp.confusion_matrix)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82TRI3sSZuAD"
      },
      "source": [
        "## Model analyseren\r\n",
        "Sommige machine learning modellen worden wel eens \"black box\" genoemd. Bijvoorbeeld de google en facebook algoritmes schijnen onbegrijpbaar te zijn voor mensen, puur het resultaat van heel veel training data in een model laden wat niet uit te lezen is.\r\n",
        "\r\n",
        "Gelukkig is een decision tree dat wel. We kunnen het getrainde model uitwerken om te zien welke volgorde van parameters het model heeft gekozen om een voorspelling te maken."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DyQYaR5aZG9"
      },
      "source": [
        "fn=list(X_train.columns)\r\n",
        "cn=['Yes', 'No']\r\n",
        "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=300)\r\n",
        "tree.plot_tree(decision_tree,\r\n",
        "               feature_names = fn, \r\n",
        "               class_names=cn,\r\n",
        "               filled = True);\r\n",
        "fig.savefig('decisiontree_visual.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JfhqR81U4w5"
      },
      "source": [
        "# Model tweaken\r\n",
        "\r\n",
        "We hebben nu een model wat een score haalt van 74% correcte voorspellingen. Dit is best oké, maar kan beter! We kunnen onze parameters aanpassen om een beter model te maken. Heb je ideeën hoe?\r\n",
        "\r\n",
        "We zouden onze data verder kunnen parsen tot andere waardes. Denk aan: Age categoriseren (bijv '0-20', '20-40', etc) (of: adult: 1 of 0). Mogelijk hebben we nu ook data meegenomen die helemaal niet zo interessant is voor ons model. In het model hierboven valt bijvoorbeeld op dat 'Embarked' vaak pas als laatste criterium wordt gebruikt. Wat gebeurt er met het model wanneer we die kolom weglaten?\r\n",
        "\r\n",
        "Daarnaast kunnen we kijken naar het reisgezelschap, had je meer kans op overleving als je alleen reist of met familie?\r\n",
        "En je [titel](https://www.kaggle.com/manuelatadvice/feature-engineering-titles)?\r\n",
        "\r\n",
        "Al deze aanpassingen kunnen we doen, het model opnieuw draaien, meer aanpassingen doen, enz. (let op, je kan deze code niet zomaar draaien nu, omdat we bijvoorbeeld de 'Name' kolom bovenin al hebben verwijderd. Wil je de data tweaken, draai dan dit notebook opnieuw vanaf het inladen van de csv file en pas de relevante stappen aan, of sla stappen over. Draai daarna het model opnieuw."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iizQRzFaP62T"
      },
      "source": [
        "# df['alone'] = np.where((df['SibSp'] == 0) & (df['Parch'] == 0), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ3o_d11FyxB"
      },
      "source": [
        "# df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vnSoou6j36y"
      },
      "source": [
        "## Onze hypotheses\r\n",
        "\r\n",
        "Laten we eens wat proberen. We hebben dit stiekem al voorbereid toen we een kopie maakten van de dataset eerder, deze gaan we nu gebruiken. We gaan even puur de kolommen gebruiken die we zelf relevant hadden verwacht: leeftijd, geslacht en klasse. Om het model super simpel te maken, splitsen we leeftijd uit in 2 categoriën: volwassen of kind. (Omdat dit een categoriale waarde is, One-Hot-Encoden we die ook meteen).\r\n",
        "Daarna draaien we het hele model nog eens, en bekijken de impact van deze parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMhO3sLfgEl-"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyZaijHzhONj"
      },
      "source": [
        "df2.loc[df2['Age'] >= 18, 'adult'] = 1\r\n",
        "df2.loc[df2['Age'] < 18, 'adult'] = 0\r\n",
        "df2.drop(['Age', 'SibSp', 'Parch', 'Fare', 'alive', 'Embarked_Q', 'Embarked_S'],axis=1, inplace=True)\r\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0s7QqeDfsnD"
      },
      "source": [
        "# split data into training features and labels\r\n",
        "X, y = df2.loc[:, df2.columns != 'Survived'], df2['Survived']\r\n",
        "\r\n",
        "# split data into train and test sets\r\n",
        "seed = 7\r\n",
        "test_size = 0.2\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\r\n",
        "\r\n",
        "decision_tree = DecisionTreeClassifier()\r\n",
        "decision_tree.fit(X_train, y_train)\r\n",
        "\r\n",
        "Y_pred = decision_tree.predict(X_test)\r\n",
        "\r\n",
        "print('score:', decision_tree.score(X_test, y_test))\r\n",
        "\r\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None),\r\n",
        "                  (\"Normalized confusion matrix\", 'true')]\r\n",
        "for title, normalize in titles_options:\r\n",
        "    disp = plot_confusion_matrix(decision_tree, X_test, y_test,\r\n",
        "                                 display_labels=['Yes', 'No'],\r\n",
        "                                 cmap=plt.cm.Blues,\r\n",
        "                                 normalize=normalize)\r\n",
        "    disp.ax_.set_title(title)\r\n",
        "\r\n",
        "    print(title)\r\n",
        "    print(disp.confusion_matrix)\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "from sklearn import tree\r\n",
        "fn=list(X_train.columns)\r\n",
        "cn=['Yes', 'No']\r\n",
        "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (5,5), dpi=300)\r\n",
        "tree.plot_tree(decision_tree,\r\n",
        "               feature_names = fn, \r\n",
        "               class_names=cn,\r\n",
        "               filled = True);\r\n",
        "fig.savefig('decisiontree_visual.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kho14zskTEd"
      },
      "source": [
        "Onze score is gestegen! We zijn van 74% naar 83% gegaan, met een veel simpeler model. Daarnaast blijkt uit de visualisatie van ons model welke waardes het meest interessant zijn. Lees het model maar eens door!\r\n",
        "\r\n",
        "We zien dat de belangrijkste keuze geslacht is. Is je geslacht man (female <= 0.5), dan hangt je overleving daarna af van je leeftijd. Is je geslacht echter vrouw, dan hangt je geslacht daarna het meest af van in welke klasse je reist, en in het geval dat je 3e klas reis, je leeftijd. Reis je als vrouw 2e of 1e klas? Dan lijkt uit het model je kans op overleving eigenlijk al heel klein! Klopt dat?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJW-NQqkn8-"
      },
      "source": [
        "# Zou jij overleven?\r\n",
        "\r\n",
        "Nog één laatste test. Zou jij de Titanic ramp overleefd hebben? Gezien jouw leeftijd en geslacht, welke klasse zou je moeten reizen om kans te hebben op overleving?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u-DWM2Hkv8j"
      },
      "source": [
        "test_jezelf = pd.DataFrame([{'adult': 1, 'female': 1, 'Pclass_2': 0, 'Pclass_3': 0}])\r\n",
        "# let op dat je Pclass_1 er bij moet denken, maar niet moet toevoegen. Één van de Pclass kolommen moet 1 zijn, de andere 2 hebben dan een 0.\r\n",
        "test_jezelf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PeK1I5mk8lm"
      },
      "source": [
        "decision_tree.predict(test_jezelf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek9YsZ0ilwl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}